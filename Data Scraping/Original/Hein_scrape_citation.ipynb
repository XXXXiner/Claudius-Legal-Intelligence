{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.select import Select\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import argparse\n",
    "import os\n",
    "from urllib.parse import urlparse, parse_qs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "1. \"date\": \"2020-2021\"\n",
    "2. \"topics\"\n",
    "3. \"citedby_cases\": 0,\n",
    "4. \"citedby_ALI\": 0,\n",
    "5. \"accessed_by\": 133,\n",
    "6. \"text\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:110: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:110: SyntaxWarning: invalid escape sequence '\\ '\n",
      "/var/folders/x9/xpdst2m52kn34r91gkghvkd00000gn/T/ipykernel_17287/2384533398.py:110: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  pattern = re.compile(\"issue\\ \\d+\")\n"
     ]
    }
   ],
   "source": [
    "LANDING_URL = \"https://heinonline.org/HOL/Welcome\"  # url for log in page\n",
    "# LANDING_URL = \"https://heinonline-org.revproxy.brown.edu/HOL/Welcome\"\n",
    "LOG_IN_WAIT_TIME = 45                                # wait time to allow us to enter log in info, unit: second\n",
    "OUTPUT_DIR = './'                                   # output directory\n",
    "\n",
    "# skip the article if its title starts with one of these stopwords\n",
    "STOPWORDS = ['table of contents', 'title page', 'index to volume']\n",
    "\n",
    "def download(args):\n",
    "    service=Service(ChromeDriverManager().install())\n",
    "\n",
    "    # Options\n",
    "    chrome_options = Options()\n",
    "\n",
    "    if args.headless:\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    \n",
    "    # driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    driver = webdriver.Safari()\n",
    "\n",
    "    # Depending on speed at which browsers load, this implicit wait time may need to be adjusted\n",
    "    driver.implicitly_wait(0)\n",
    "\n",
    "    parent = driver.window_handles[0]\n",
    "    driver.switch_to.window(parent)\n",
    "\n",
    "    # Go to log in page and manually enter log in info\n",
    "    landing_url = LANDING_URL\n",
    "    driver.get(landing_url)\n",
    "    time.sleep(LOG_IN_WAIT_TIME)\n",
    "\n",
    "    journal_lib = driver.find_element(By.LINK_TEXT, \"Law Journal Library\")\n",
    "    journal_lib.click()\n",
    "\n",
    "    # Change the first letter variable to be the first letter of the journal you want\n",
    "    journal_name = args.journal_name\n",
    "    # tab_index = ord(journal_name[0].lower()) - ord('a') + 1\n",
    "    first_letter = driver.find_element(By.LINK_TEXT, journal_name[0].upper())\n",
    "    # first_letter = driver.find_element(By.XPATH, '//*[@id=\"headerWrapper\"]/div/a[{}]'.format(tab_index))\n",
    "    first_letter.click()\n",
    "\n",
    "    # Go to the page of this journal\n",
    "    journal = driver.find_element(By.LINK_TEXT, journal_name)\n",
    "    journal.click()\n",
    "\n",
    "    # List of volumes to be scraped\n",
    "    vol_number_list=[str(x) for x in range(args.start_vol, args.end_vol + 1)]\n",
    "\n",
    "    # Data dict for this journal\n",
    "    data = {}\n",
    "\n",
    "    # Create output directory\n",
    "    output_dir = OUTPUT_DIR + args.journal_name + '/'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # i is volume number\n",
    "        for vol_num in vol_number_list:\n",
    "            volume_list = driver.current_url\n",
    "            volumes=driver.find_elements(By.PARTIAL_LINK_TEXT, vol_num+\" \")\n",
    "\n",
    "            # Volume number = year\n",
    "            if not volumes:\n",
    "                volumes=driver.find_elements(By.PARTIAL_LINK_TEXT, vol_num)\n",
    "            \n",
    "            print([volume.text for volume in volumes])\n",
    "            vol = volumes[len(volumes)-1]\n",
    "\n",
    "            # Check volume number\n",
    "            true_vol_num = vol.text.split(' ')[0].strip()\n",
    "            \n",
    "            # If wrong volume number, log errors\n",
    "            if vol_num != true_vol_num:\n",
    "                log_filename = '{}_vol{}_error_wrong_vol_num.txt'.format(journal_name.replace(' ', '_'), vol_num)\n",
    "                with open(output_dir + log_filename, 'a') as f:\n",
    "                    f.write(vol_num + '\\t' + vol.text)\n",
    "                    f.write('\\n')\n",
    "                continue\n",
    "\n",
    "            vol.click()\n",
    "            data[journal_name]= {vol_num:[]}\n",
    "            elements = driver.find_elements(By.CLASS_NAME, 'atocpage')\n",
    "\n",
    "            # List of ids of html elements that correspond to articles\n",
    "            id_list = [element.get_attribute('id') for element in elements]\n",
    "            for id in id_list:\n",
    "                # No link\n",
    "                if journal_name == \"St. John's Law Review\" and vol_num == \"80\" and id == \"20\":\n",
    "                    continue\n",
    "                \n",
    "                section = driver.find_element(By.XPATH, '//*[@id=\"'+id+'\"]')\n",
    "                article = section.find_element(By.XPATH, './div/a[1]')\n",
    "                article.click()\n",
    "                article_url = driver.current_url\n",
    "                # article = section.find_element(By.XPATH, './div/a[1]')\n",
    "                # article.click()\n",
    "                time.sleep(1)\n",
    "\n",
    "                # Get entry type\n",
    "                entry_type = section.find_element(By.XPATH, './i[1]').text\n",
    "                if entry_type == '':\n",
    "                    entry_type = 'Article'\n",
    "                \n",
    "                # Skip table of contents, title page etc.\n",
    "                if any(entry_type.lower().startswith(stopword) for stopword in STOPWORDS):\n",
    "                    continue\n",
    "                \n",
    "                # Skip issues\n",
    "                pattern = re.compile(\"issue\\ \\d+\")\n",
    "                if bool(re.match(pattern, entry_type.lower())):\n",
    "                    continue\n",
    "\n",
    "                # Skip non articles\n",
    "                if entry_type != 'Article':\n",
    "                    continue\n",
    "\n",
    "                # Data dict for this article\n",
    "                article_data = {}\n",
    "                \n",
    "\n",
    "                # Get title\n",
    "                # Title is the textContent of the second child node of the section element\n",
    "                title = driver.execute_script(\"return arguments[0].childNodes[1].textContent.trim();\", section)\n",
    "                article_data[\"title\"] = title\n",
    "                article_data[\"entry_type\"] = entry_type\n",
    "\n",
    "                print('volume: ' + str(vol_num) + '; ' + entry_type + '; ' + str(title))\n",
    "\n",
    "                # Get authors\n",
    "                authors=None\n",
    "                if(len(section.find_elements(By.XPATH,'./a'))>1):\n",
    "                    title_author = section.text.split(\"\\n\")\n",
    "                    authors = title_author[1].split(\";\")\n",
    "                    authors = [author.strip() for author in authors]\n",
    "                article_data[\"authors\"] = authors\n",
    "\n",
    "                # Get date and url\n",
    "                article_data[\"url\"] = article_url\n",
    "\n",
    "\n",
    "\n",
    "                # Get citation info\n",
    "                try:\n",
    "                    citation = driver.find_element(By.CLASS_NAME,'scholarcheck_icon')\n",
    "                    citation.click()\n",
    "                    cited_by_articles_count = driver.find_element(By.ID, 'cite_counts')\n",
    "                    cited_by_articles_count = int(cited_by_articles_count.text)\n",
    "                    article_data[\"citedby_articles\"] = cited_by_articles_count\n",
    "\n",
    "                    citing_articles = []\n",
    "                    if cited_by_articles_count == 0:\n",
    "                        article_data[\"citing_articles\"] = citing_articles\n",
    "                    else:\n",
    "                        cited_by_articles_link = driver.find_element(By.ID,'cite_countsd')\n",
    "                        cited_by_articles_link.click()\n",
    "\n",
    "                        time.sleep(1)\n",
    "                        scroll_count = 0\n",
    "                        while \"No More Results\" not in driver.page_source:\n",
    "                            # Scroll down\n",
    "                            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                            print('Scrolling...' + str(scroll_count))\n",
    "                            scroll_count += 1\n",
    "                            time.sleep(15)\n",
    "\n",
    "                        \n",
    "                        # # Get scroll height after first time page load\n",
    "                        # last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                        # while True:\n",
    "                        #     # Scroll down to bottom\n",
    "                        #     driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                        #     # Wait to load page / use a better technique like `waitforpageload` etc., if possible\n",
    "                        #     time.sleep(5)\n",
    "                        #     # Calculate new scroll height and compare with last scroll height\n",
    "                        #     new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                        #     if new_height == last_height:\n",
    "                        #         break\n",
    "                        #     last_height = new_height\n",
    "\n",
    "                        time.sleep(2)\n",
    "                        search_results = driver.find_elements(By.XPATH, \"//div[contains(@class, 'lucene_search_result_b')]\")\n",
    "\n",
    "                        for search_result in search_results:\n",
    "                            try:\n",
    "                                citing_article_name = search_result.find_element(By.XPATH, \"./dt[1]\").text\n",
    "                            except:\n",
    "                                citing_article_name = 'N/A'\n",
    "                            \n",
    "                            try:\n",
    "                                citing_article_journal = search_result.find_element(By.XPATH, \"./dt[3]\").text\n",
    "                            except:\n",
    "                                citing_article_journal = 'N/A'\n",
    "                            \n",
    "                            try:\n",
    "                                citing_article_authors = search_result.find_element(By.XPATH, \"./dt[4]\").text\n",
    "                            except:\n",
    "                                citing_article_authors = 'N/A'\n",
    "                            \n",
    "                            try:\n",
    "                                citing_article_ref = search_result.find_element(By.XPATH, \"./dt[6]\").text\n",
    "                            except:\n",
    "                                citing_article_ref = 'citing_article_ref'\n",
    "                            \n",
    "                            citing_articles.append(\n",
    "                                {\n",
    "                                    'citing_article_name': citing_article_name,\n",
    "                                    'citing_article_journal': citing_article_journal,\n",
    "                                    'citing_article_authors': citing_article_authors,\n",
    "                                    'citing_article_ref': citing_article_ref\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "                        article_data[\"citing_articles\"] = citing_articles\n",
    "                        driver.back()\n",
    "                    \n",
    "                    citation.click()\n",
    "                    time.sleep(1)\n",
    "                except Exception as e:\n",
    "                    article_data[\"citedby_articles\"] = 'N/A'\n",
    "                    article_data[\"citing_articles\"] = []\n",
    "                    print(\"Error: \" + article_url)\n",
    "                    print(e)\n",
    "                    \n",
    "                    # Log errors\n",
    "                    log_filename = '{}_vol{}_error_citation.txt'.format(journal_name.replace(' ', '_'), vol_num)\n",
    "                    with open(output_dir + log_filename, 'a') as f_error:\n",
    "                        f_error.write(article_url)\n",
    "                        f_error.write('\\n')\n",
    "\n",
    "                # Write data dict to json\n",
    "                data[journal_name][vol_num].append(article_data)\n",
    "                json_filename = '{}_vol{}.json'.format(journal_name.replace(' ', '_'), vol_num)\n",
    "                with open(output_dir + json_filename, 'w') as f:\n",
    "                    json.dump(data, f, indent=4)\n",
    "                \n",
    "            # Get back to the volume page\n",
    "            driver.get(volume_list)\n",
    "            time.sleep(2)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('*'*20)\n",
    "        \n",
    "# Convert html list to dictionary\n",
    "def html_list_to_dict(html_list):\n",
    "    result = {}\n",
    "    for li in html_list.find_all(\"li\", recursive=False):\n",
    "        key = next(li.stripped_strings)\n",
    "        html_list = li.find(\"ol\")\n",
    "        if html_list:\n",
    "            tmp = html_list_to_dict(html_list)\n",
    "            if tmp: \n",
    "                result[key] = tmp\n",
    "            else:\n",
    "                result[key] = None\n",
    "        else:\n",
    "            result[key] = None\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--journal_name', type=str,default = \"UC Irvine Law Review\")\n",
    "parser.add_argument('--journal_abbrev', type=str,default =\"ucirvlre\")\n",
    "parser.add_argument('--start_vol', type=int,default = 12 ) # scraping starts at this volume number\n",
    "parser.add_argument('--end_vol', type=int,default = 13)   # scraping ends at this volume number\n",
    "parser.add_argument('--headless', action='store_true')  # scrape without opening browser window\n",
    "\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"all journals in database.xlsx\")\n",
    "#filter the journals that are not yet scraped and abbrev is not NAN\n",
    "data = data[data[\"Progress\"] != \"Scraped\"]\n",
    "data = data[data[\"Abbrev\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Journal</th>\n",
       "      <th>Progress</th>\n",
       "      <th>Abbrev</th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>Start Year</th>\n",
       "      <th>Current Year</th>\n",
       "      <th>Note</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama Law Review</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bamalr</td>\n",
       "      <td>52</td>\n",
       "      <td>74</td>\n",
       "      <td>2001</td>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UC Irvine Law Review</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ucirvlre</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2011</td>\n",
       "      <td>2023</td>\n",
       "      <td>Previous name was wrong</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>American University Law Review</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aulr</td>\n",
       "      <td>50</td>\n",
       "      <td>72</td>\n",
       "      <td>2001</td>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arizona Law Review</td>\n",
       "      <td>NaN</td>\n",
       "      <td>arz</td>\n",
       "      <td>42</td>\n",
       "      <td>65</td>\n",
       "      <td>2000</td>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arizona State Law Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>arzjl</td>\n",
       "      <td>32</td>\n",
       "      <td>55</td>\n",
       "      <td>2000</td>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>Yale Journal of Law and Feminism</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yjfem</td>\n",
       "      <td>12</td>\n",
       "      <td>34</td>\n",
       "      <td>2001</td>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>Yale Journal of Law and Technology</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yjolt</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>2002</td>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>Yale Journal on Regulation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yjor</td>\n",
       "      <td>17</td>\n",
       "      <td>40</td>\n",
       "      <td>2000</td>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>Yale Law &amp; Policy Review</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yalpr</td>\n",
       "      <td>19</td>\n",
       "      <td>41</td>\n",
       "      <td>2001</td>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>Yale Law Journal Forum</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yljfor</td>\n",
       "      <td>121</td>\n",
       "      <td>133</td>\n",
       "      <td>2011</td>\n",
       "      <td>2023</td>\n",
       "      <td>115-120 missing</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Journal  Progress    Abbrev  Start  End  \\\n",
       "0                    Alabama Law Review       NaN    bamalr     52   74   \n",
       "1                  UC Irvine Law Review       NaN  ucirvlre      1   13   \n",
       "2        American University Law Review       NaN      aulr     50   72   \n",
       "3                    Arizona Law Review       NaN       arz     42   65   \n",
       "4             Arizona State Law Journal       NaN     arzjl     32   55   \n",
       "..                                  ...       ...       ...    ...  ...   \n",
       "207    Yale Journal of Law and Feminism       NaN     yjfem     12   34   \n",
       "208  Yale Journal of Law and Technology       NaN     yjolt      3   24   \n",
       "209          Yale Journal on Regulation       NaN      yjor     17   40   \n",
       "210            Yale Law & Policy Review       NaN     yalpr     19   41   \n",
       "211             Yale Law Journal Forum        NaN    yljfor    121  133   \n",
       "\n",
       "     Start Year  Current Year                     Note  Url  \n",
       "0          2001          2023                      NaN  NaN  \n",
       "1          2011          2023  Previous name was wrong  NaN  \n",
       "2          2001          2023                      NaN  NaN  \n",
       "3          2000          2023                      NaN  NaN  \n",
       "4          2000          2023                      NaN  NaN  \n",
       "..          ...           ...                      ...  ...  \n",
       "207        2001          2023                      NaN  NaN  \n",
       "208        2002          2023                      NaN  NaN  \n",
       "209        2000          2023                      NaN  NaN  \n",
       "210        2001          2023                      NaN  NaN  \n",
       "211        2011          2023          115-120 missing  NaN  \n",
       "\n",
       "[212 rows x 9 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alabama Law Review\n",
      "bamalr\n",
      "52\n",
      "74\n",
      "Message: Could not create a session: The session timed out while connecting to a Safari instance.\n",
      "\n",
      "Error in scraping Alabama Law Review\n",
      "UC Irvine Law Review\n",
      "ucirvlre\n",
      "1\n",
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x9/xpdst2m52kn34r91gkghvkd00000gn/T/ipykernel_17287/1349289243.py:18: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Error' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  data.at[index, \"Progress\"] = \"Error\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: Could not create a session: The session timed out while connecting to a Safari instance.\n",
      "\n",
      "Error in scraping UC Irvine Law Review\n",
      "American University Law Review\n",
      "aulr\n",
      "50\n",
      "72\n",
      "Message: Could not create a session: The session timed out while connecting to a Safari instance.\n",
      "\n",
      "Error in scraping American University Law Review\n",
      "Arizona Law Review\n",
      "arz\n",
      "42\n",
      "65\n",
      "Message: Could not create a session: The session timed out while connecting to a Safari instance.\n",
      "\n",
      "Error in scraping Arizona Law Review\n",
      "Arizona State Law Journal\n",
      "arzjl\n",
      "32\n",
      "55\n",
      "Message: Could not create a session: The session timed out while connecting to a Safari instance.\n",
      "\n",
      "Error in scraping Arizona State Law Journal\n",
      "Arkansas Law Review\n",
      "arklr\n",
      "53\n",
      "76\n",
      "Message: Could not create a session: The session timed out while connecting to a Safari instance.\n",
      "\n",
      "Error in scraping Arkansas Law Review\n",
      "Baylor Law Review\n",
      "baylr\n",
      "52\n",
      "75\n",
      "Message: Could not create a session: The session timed out while connecting to a Safari instance.\n",
      "\n",
      "Error in scraping Baylor Law Review\n",
      "Boston College Law Review\n",
      "bclr\n",
      "42\n",
      "64\n",
      "Message: Could not create a session: The session timed out while connecting to a Safari instance.\n",
      "\n",
      "Error in scraping Boston College Law Review\n",
      "Boston University Law Review\n",
      "bulr\n",
      "80\n",
      "103\n",
      "Message: Could not create a session: The session timed out while connecting to a Safari instance.\n",
      "\n",
      "Error in scraping Boston University Law Review\n",
      "Brigham Young University Law Review\n",
      "byulr\n",
      "2000\n",
      "49\n",
      "Message: Could not create a session: The session timed out while connecting to a Safari instance.\n",
      "\n",
      "Error in scraping Brigham Young University Law Review\n",
      "Brooklyn Law Review\n",
      "brklr\n",
      "66\n",
      "88\n",
      "Message: Could not create a session: The session timed out while connecting to a Safari instance.\n",
      "\n",
      "Error in scraping Brooklyn Law Review\n",
      "Buffalo Law Review\n",
      "buflr\n",
      "48\n",
      "71\n",
      "Message: Could not create a session: The session timed out while connecting to a Safari instance.\n",
      "\n",
      "Error in scraping Buffalo Law Review\n",
      "California Law Review\n",
      "calr\n",
      "86\n",
      "111\n",
      "Message: Could not create a session: The session timed out while connecting to a Safari instance.\n",
      "\n",
      "Error in scraping California Law Review\n",
      "Cardozo Law Review\n",
      "cdozo\n",
      "35\n",
      "44\n",
      "Message: Could not create a session: The session timed out while connecting to a Safari instance.\n",
      "\n",
      "Error in scraping Cardozo Law Review\n",
      "Case Western Reserve Law Review\n",
      "cwrlrv\n",
      "51\n",
      "73\n"
     ]
    }
   ],
   "source": [
    "# read the data row by row\n",
    "for index, row in data.iterrows():\n",
    "    try:\n",
    "        args.journal_name = row[\"Journal\"].strip()\n",
    "        args.journal_abbrev = row[\"Abbrev\"]\n",
    "        args.start_vol = int(row[\"Start\"])\n",
    "        args.end_vol = int(row[\"End\"])\n",
    "        print(args.journal_name)\n",
    "        print(args.journal_abbrev)\n",
    "        print(args.start_vol)\n",
    "        print(args.end_vol)\n",
    "        download(args)\n",
    "        data.at[index, \"Progress\"] = \"Scraped\"\n",
    "        # data.to_excel(\"Huangrui Journals Progress.xlsx\", index=False)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Error in scraping \" + args.journal_name)\n",
    "        data.at[index, \"Progress\"] = \"Error\"\n",
    "data.to_excel(\"Huangrui Journals Progress.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel(\"Huangrui Journals Progress.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
